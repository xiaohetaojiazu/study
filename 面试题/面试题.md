## 什么是零拷贝

从字面上理解，是不需要将数据从一个存储区域复制到另一个存储区域。最早的零拷贝定义来源于 Linux 系统的 sendfile 方法逻辑。

> 在 Linux2.4 内核中，sendfile 系统调用方法，可以将磁盘数据通过 DMA 拷贝到内核态 Buffer 后，再通过 DMA 拷贝到 Socket Buffer，无需 CPU 拷贝，这个过程称之为零拷贝。

零拷贝并不是不需要拷贝，而是减少了不必要的拷贝。

零拷贝技术有以下两点优势：

- 减少数据拷贝和共享总线操作的次数。消除传输数据在存储器之间不必要的中间拷贝，从而有效地提高数据传输效率；
- 零拷贝技术减少了用户态和内核态之间因为上下文切换而带来的性能开销；

常见的零拷贝方式有 mmap、sendfile 和 splice 等。

### 传统 IO 拷贝流程

![image-20231113222848898](面试题.pic/image-20231113222848898.png)

如上图，传统的 IO 读写流程如下∶

1. 应用程序调用 read 方法，CPU 由用户态切换到内核态，通过 DMA 技术将硬盘中的数据拷贝到内核缓冲区；
2. DMA 拷贝完成后，会发起一个中断通知 CPU，然后 CPU 会将内核缓冲区的数据拷贝到应用程序缓冲区；
3. 数据拷贝完成后，CPU 由用户态切换到内核态，应用程序线程进行业务处理；
4. 应用程序调用 write 方法对请求进行响应，CPU 由用户态切换到内核态，同时将应用缓冲区的数据拷贝到 socket 网络发送缓冲区；
5. 拷贝完成后，再由网络适配器通过 DMA 技术将 socket 网络发送缓冲区中的数据拷贝到网卡；
6. 拷贝完成后，CPU会将内核态切换回用户态，同时网卡会将数据发送出去；

整个过程,共产生 4 次数据拷贝和 4 次上下文切换：

- 4 次数据拷贝：2 次 DMA 拷贝，2 次 CPU 拷贝；
- 4 次上下文切换：read() / write() 调用各 2 次用户态和内核态的切换；

其中，用户空间和内核空间的 2 次 CPU 拷贝，作用是为了安全和缓存。那么，是否存在这么—种技术，既能保证操作系统的安全，
又能减少这种看似没有必要的 CPU 拷贝呢?接下来让我们看下 mmap 内存映射技术。

### mmap 内存映射

mmap 全称 memory map，翻译过来就是内存映射。它可以将内核态和用户态的内存映射在一起，以避免数据的来回拷贝。实现了这样的映射关系后，进程就可以采用指针的方式来操作这一段内存，系统会自动回写脏页面到对应的文件磁盘上，即完成了对文件的操作而不必再调用 read，write 等系统调用函数。相反，内核空间对这段内存区域的修改也会直接反映到用户空间，从而可以实现不同进程间的文件共享。

一般来说，mmap 内存映射技术可以替代 read 方法，即把传统的 read / write 替换为 mmap + write方式，替换后的流程如下图：

![image-20231113224110949](面试题.pic/image-20231113224110949.png)

很明显，这种方案整个过程，会产生 3 次数据拷贝和 4 次上下文切换：

- 3 次数据拷贝：2 次 DMA 拷贝，1 次 CPU 拷贝；
- 4 次上下文切换：mmap() / write() 调用各 2 次用户态和内核态的切换；

情况似乎得到了一些改善，但是对于仅做文件传输，应用程序并不会数据做任何的修改的场景，此方案还是有可优化的空间的。接下来让我们看下 sendfile。

### sendfile

我们可以通过 sendfile 的方式，只做文件传输，而不通过用户态对数据进行任何干预。流程图如下：

![image-20231113224359744](面试题.pic/image-20231113224359744.png)

此时，我们发现，整个过程变为了 3 次数据拷贝和 2 次上下文切换：

- 3 次数据拷贝：2 次 DMA 拷贝，1 次 CPU 拷贝；
- 2 次上下文切换：sendfile() 的调用产生的 2 次用户态和内核态的切换；

还是很不甘心，有木有？我们注意下第三步的中 CPU 拷贝，既然都在内核态，那么有木有方案能把它优化掉呢？接下里让我们看下
sendfile + DMA scatter/gather 的拷贝流程。

### sendfile + DMA scatter/gather

在 Linux 2.4 内核版本中，对 sendfile 系统方法做了优化升级，引入 SG-DMA 技术，当然该技术需要 DMA 控制器硬件的支持。DMA gather 读取内核缓冲区(page cache)中的数据描述信息，包括内存地址和偏移量，并将其记录到 socket cache 中，DMA 根据这些数据描述信息，可以将内核缓冲区的数据直接拷贝到网卡，从而减少了一次 CPU 拷贝的过程。具体流程图如下：

![image-20231113224937148](面试题.pic/image-20231113224937148.png)

由上图可以看出，整个过程变为了 2 次数据拷贝和 2 次上下文切换。似乎一切近乎完美了，但是这种方式需要硬件的支持，有木有更完美的方式呢？最后，让我们看一下 splice。

### splice

在 Linux 2.6.17 内核版本中，引入了 splice 系统调用方法，和 sendfile 方法不同的是，splice 不需要硬件支持。

它将数据从磁盘读取到 OS 内核缓冲区后，内核缓冲区和 socket 缓冲区之间通过建立管道来传输数据，避免了两者之间的 CPU 拷贝操作。

整个拷贝过程，可以用如下流程图来描述：

![image-20231113225304861](面试题.pic/image-20231113225304861.png)

同样，该方案的整个过程也是 2 次数据拷贝和 2 次上下文切换。

### 拓展知识

- **什么是 DMA？**

  DMA，英文全称是 Direct Memory Access，即直接内存访问。DMA 本质上是一块主板上独立的芯片，允许外设设备和内存存储器之间直接进行 IO 数据传输，其过程不需要CPU 的参与。

- **内核空间和用户空间**

  操作系统的核心是内核，与普通的应用程序不同，它可以访问受保护的内存空间，也有访问底层硬件设备的权限。

  为了避免用户进程直接操作内核，保证内核安全，操作系统将虚拟内存划分为两部分，一部分是内核空间(Kernel-space)，一部分是用户空间(User-space)。在 Linux 系统中，内核模块运行在内核空间，对应的进程处于内核态；而用户程序运行在用户空间，对应的进程处于用户态。

  内核空间总是驻留在内存中，它是为操作系统的内核保留的。应用程序是不允许直接在该区域进行读写或直接调用内核代码定义的函数的。

  当启动某个应用程序时，操作系统会给应用程序分配一个单独的用户空间，其实就是一个用户独享的虚拟内存，每个普通的用户进程之间的用户空间是完全隔离的、不共享的，当用户进程结束的时候，用户空间的虚拟内存也会随之释放。

  同时处于用户态的进程不能访问内核空间中的数据，也不能直接调用内核函数。如果要调用系统资源，就要将进程切换到内核态，由内核程序来进行操作。

- **Java 中零拷贝的实现**

  Linux 提供的零拷贝技术，Java 并不是全部支持，目前只支持以下 2 种：

  - mmap
  - sendfile

- **Java NIO 对 mmap 的支持**

  Java NIO 有一个 MappedByteBuffer 的类，可以用来实现内存映射。它的底层是调用了 Linux 内核的 mmap 的 API。代码实现如下:

  ```java
  ```

- **Kafka 中的零拷贝实现**

  Kafka 两个重要过程都使用了零拷贝技术，一是 Producer 生产的数据存到 broker，二是 Consumer 从 broker 读取数据。

  Producer 生产的数据持久化到 broker，broker 里采用 mmap 文件映射，实现顺序的快速写入；

  Customer 从 broker 读取数据，broker 里采用 sendfile，将磁盘文件读到 OS 内核缓冲区后，直接转到 socket buffer 进行网络发送。

- **Netty 中的零拷贝实现**

  Netty 的零拷贝主要包含三个方面：

  - 在网络通信上，Netty 的接收和发送 ByteBuffer 采用 DIRECT BUFFERS，使用堆外直接内存进行 Socket 读写，不需要进行字节缓冲区的二次拷贝。
  - 在缓存操作上，Netty 提供了 CompositeByteBuf 类，它可以将多个 ByteBuf 合并为一个逻辑上的 ByteBuf，避免了各个 ByteBuf 之间的拷贝。通过 wrap 操作，我们可以将 byte[] 数组、ByteBuf、ByteBuffer 等包装成一个 Netty ByteBuf 对象，进而避免了拷贝操作。ByteBuf 支持 slice 操作，因此可以将 ByteBuf 分解为多个共享同一个存储区域的 ByteBuf，避免了内存的拷贝。
  - 在文件传输上，Netty 的通过 FileRegion 包装的 FileChannel.tranferTo 实现文件传输，它可以直接将文件缓冲区的数据发送到目标 Channel，避免了传统通过循环 write 方式导致的内存拷贝问题。



## 介绍一下 Netty

Netty 是由 JBOSS 提供的一个 Java 开源框架。它提供异步的、基于事件驱动的网络应用程序框架，可以快速开发高性能、高可靠性的网络 IO 程序。Netty 的应用场景主要是网络通信，比如用作 RPC 框架的通信工具、即时通讯的系统、实时消息推送的系统等。

Netty 是一个基于 NIO 的网络编程框架，相比于 Java 中自带的 NIO，它具备以下优势：

1. API 使用简单，开发门槛低；
2. 功能强大：Netty 预置了多种编解码功能，支持多种主流协议；
3. 扩展性强：通过 ChannelHandler 可以进行灵活的拓展，具备很强的定制能力；
4. 高性能：与其他业界主流 NIO 框架相比，Netty 综合性能更优，主要体现在吞吐量更高、延迟更低、减少资源消耗以及最小化不必要的内存复制等；
5. 社区活跃：版本迭代周期短，发现的BUG可以被及时修复；
6. 成熟稳定：Netty 修复了 Java NlO 已被发现的所有 BUG，另外，它经受了很多知名项目的使用和考验，比如 Dubbo / Zookeeper / ES / RocketMQ / Spark / Hadoop 等。

### 为什么 Netty 使用 NIO 而不是 AIO？

首先，让我们看下 Netty 的作者对这个问题的回答：

> Not faster than NIO(epoll) on unix systems (which is true)
>
> There is no daragram suppport
>
> Unnecessary threading model (too much abstraction without usage)

针对这个问题，总结以下三点：

- 在 Linux 系统上，AIO 的底层实现仍使用 EPOLL，在性能上没有明显的优势，而且被 JDK 封装了一层不容易深度优化；
- Netty 整体架构是 reactor 模型，而 AlO 是 proactor 模型，糅合在一起会非常混乱；
- AIO 还有个缺点就是接收数据需要预先分配缓存，而不是 NIO 那种需要接收时才需要分配缓存，所以对连接数量非常大但流量小的情况，内存浪费很多。

### 为什么不用 Netty 5？

Netty 5 已经停止开发了。
